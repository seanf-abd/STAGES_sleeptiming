---
title: "STAGES PSG"
author: "Seán F"
date: "`r Sys.Date()`"
output: html_document
---
```{r Load Libraries, include = FALSE}
library(lubridate)
library(readr)
library(tidyverse)  
library(ggplot2)
library(ggdist)
library(gghalves)
library(dplyr)
library(readxl)
library(tidyr)
library(stringr)
library(jsonlite)
```
#Load Data and Initial Cleaning
```{r Load data, include=FALSE}
#dirs need to be changed dependin on file locations
PSG_folder <- "C:/Users/sefarrell/Documents/STAGES/PSG_Scores"
id_file <- "C:/Users/sefarrell/Downloads/sleep_timing_measures/sleep_circular_stddev_HOURS.csv"

subject_ids <- read.csv(id_file, stringsAsFactors = FALSE) %>%
  pull(subject_id) %>%
  unique() %>%
  as.character()

all_psg_files <- list.files(PSG_folder, pattern = "\\.csv$", full.names = TRUE)
psg_file_ids <- str_remove(basename(all_psg_files), "\\.csv$")
valid_psg_files <- all_psg_files[psg_file_ids %in% subject_ids]

batch_size <- 50
batches <- split(valid_psg_files, ceiling(seq_along(valid_psg_files)/batch_size))

results_list <- list()
batch_counter <- 1

#Track exclusions
failed_files <- character(0)
no_event_files <- character(0)
processed_ids <- character(0)
all_attempted_ids <- character(0)

for (batch in batches) {
  cat("Processing batch", batch_counter, "of", length(batches), "\n")
  for (file in batch) {
    this_id <- str_remove(basename(file), "\\.csv$")
    all_attempted_ids <- c(all_attempted_ids, this_id)
    
    lines <- tryCatch({
      read_lines(file)
    }, error = function(e) {
      cat("Completely unreadable file:", file, "--", e$message, "\n")
      failed_files <<- c(failed_files, this_id)
      return(NULL)
    })
    if (is.null(lines) || length(lines) < 2) {
      cat("Empty or unreadable file for", this_id, "\n")
      failed_files <- c(failed_files, this_id)
      next
    }
    
    header <- strsplit(lines[1], ",")[[1]]
    max_cols <- max(sapply(strsplit(lines, ","), length))
    data <- do.call(rbind, lapply(lines[-1], function(x) {
      vals <- strsplit(x, ",")[[1]]
      length(vals) <- max_cols
      vals
    }))
    cn <- c(header, paste0("extra_", seq_len(max_cols - length(header))))
    length(cn) <- max_cols
    colnames(data) <- cn
    data <- as.data.frame(data, stringsAsFactors = FALSE)
    
    #Now extract event columns from 3rd onward
    event_start <- 3
    event_cols <- seq(event_start, ncol(data))
    data$Event_Combined <- apply(data[, event_cols, drop = FALSE], 1, function(row) {
      paste(na.omit(trimws(as.character(row))), collapse = " ")
    })
    data$Event_clean <- ifelse(!is.na(data[[event_start]]) & trimws(data[[event_start]]) != "",
                               trimws(data[[event_start]]),
                               trimws(data$Event_Combined))
    
    valid_events <- c("Wake", "Stage1", "Stage2", "Stage3", "REM")
    data <- data[tolower(data$Event_clean) %in% tolower(valid_events), ]
    if (nrow(data) == 0) {
      cat("No valid events for", this_id, "-- unique Events (after cleaning):", paste(unique(data$Event_clean), collapse=", "), "\n")
      no_event_files <- c(no_event_files, this_id)
      results_list[[this_id]] <- NULL
      next
    }
    
    #Standardize duration column
    duration_names <- names(data)[grepl("duration", names(data), ignore.case = TRUE)]
    if (length(duration_names) > 0) {
      suppressWarnings({
        data$Duration_seconds <- as.numeric(data[[duration_names[1]]])
      })
    } else {
      data$Duration_seconds <- NA
    }
    
    data$Subject_ID <- this_id
    results_list[[this_id]] <- data
    processed_ids <- c(processed_ids, this_id)
    cat("Processed:", this_id, "(", nrow(data), "events)\n")
  }
  batch_counter <- batch_counter + 1
  cat("End of batch\n")
}
results_list <- results_list[!sapply(results_list, is.null)]

if (length(results_list) > 0) {
  common_names <- Reduce(union, lapply(results_list, names))
  for (i in seq_along(results_list)) {
    for (col in common_names) {
      if (!(col %in% names(results_list[[i]]))) {
        results_list[[i]][[col]] <- NA
      }
      if (col == "Duration_seconds") {
        suppressWarnings({
          results_list[[i]][[col]] <- as.numeric(results_list[[i]][[col]])
        })
      }
    }
    results_list[[i]] <- results_list[[i]][, common_names]
  }
  all_psg_data <- bind_rows(results_list)
  cat("Total subjects processed:", length(results_list), "\n")
} else {
  cat("No valid PSG data found.\n")
}

length(unique(clean_psg_trim$Subject_ID))

#Missing log 
missing_ids <- setdiff(subject_ids, processed_ids)
cat("\n==== Summary of Exclusions ====\n")
cat(length(missing_ids), "subject IDs missing from processed data.\n")
cat(length(failed_files), "files were unreadable/empty.\n")
cat(length(no_event_files), "files had no valid events.\n\n")

if (length(missing_ids) > 0) writeLines(missing_ids, "missing_ids.txt")
if (length(failed_files) > 0) writeLines(failed_files, "failed_files.txt")
if (length(no_event_files) > 0) writeLines(no_event_files, "no_event_files.txt")

print(missing_ids)

#Final clean, use this df throughout .Rmd updating as go
clean_psg_trim <- all_psg_data %>%
  transmute(
    Subject_ID = Subject_ID,
    Start_Time = `Start Time`,
    Event = Event_clean,
    time_sec = period_to_seconds(hms(`Start Time`)),
    day_shift = c(0, cumsum(diff(period_to_seconds(hms(`Start Time`))) < 0)),
    Date = as.Date("2000-01-01") + day_shift,
    Start_Time_full = as.POSIXct(paste(Date, `Start Time`), tz = "UTC"),
    Next_Time_full = lead(Start_Time_full),
    Computed_Duration_sec = as.numeric(difftime(lead(Start_Time_full), Start_Time_full, units = "secs")),
    Duration_seconds = Duration_seconds #keep this for logic below
  ) %>%
  group_by(Subject_ID) %>%
  mutate(
    Computed_Duration_sec = if_else(
      row_number() == n(),
      if_else(
        Event %in% c('Stage1','Stage2','Stage3','REM') & !is.na(Duration_seconds) & Duration_seconds > 0,
        Duration_seconds,
        NA_real_
      ),
      Computed_Duration_sec
    )
  ) %>%
  ungroup() %>%
  select(
    Subject_ID,
    Start_Time,
    Start_Time_full,
    Event,
    Computed_Duration_sec
  )
```
#Check for non-nocturnal PSG's
```{r Nigth PSG}
#Identify subjects whose PSG starts in the 08:00–17:00 window and exclude
bad_subjects <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(start_hour = hour(Start_Time_full)) %>%
  filter(start_hour >= 8 & start_hour < 18) %>%
  pull(Subject_ID)

#Overwrite clean_psg_trim to remove these subjects
clean_psg_trim <- clean_psg_trim %>%
  filter(!Subject_ID %in% bad_subjects)
message("Removed subject(s): ", paste(bad_subjects, collapse = ", "))
message("clean_psg_trim now contains ", n_distinct(clean_psg_trim$Subject_ID), " subjects.")
```
#WASO function
```{r WASO, echo=FALSE}
#Function to calculate WASO in seconds and mins
calculate_WASO <- function(df) {
  sleep_stages <- c('Stage1', 'Stage2', 'Stage3', 'REM')
  sleep_onset_row <- which(df$Event %in% sleep_stages)[1]
  sleep_offset_row <- tail(which(df$Event %in% sleep_stages), 1)
  if (is.na(sleep_onset_row) || is.na(sleep_offset_row) || sleep_offset_row <= sleep_onset_row) {
    return(tibble(Subject_ID = df$Subject_ID[1], waso_sec = NA, waso_min = NA))
  }

  wake_rows <- which(
    df$Event == "Wake" &
      seq_len(nrow(df)) > sleep_onset_row &
      seq_len(nrow(df)) < sleep_offset_row
  )

  waso_sec <- sum(df$Computed_Duration_sec[wake_rows], na.rm = TRUE)
  waso_min <- waso_sec / 60

  tibble(Subject_ID = df$Subject_ID[1], waso_sec = waso_sec, waso_min = waso_min)
}

#WASO calculation
waso_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ calculate_WASO(.x)) %>%
  ungroup()
#count subjects above threshold, in this case it was way too high to exclude them all
n_above_threshold <- waso_table %>%
  filter(!is.na(waso_min) & waso_min > 31) %>%
  nrow()
cat("", n_above_threshold, "subjects have WASO > 31 minutes\n")

```
#Total sleep time
```{r TST, echo=FALSE}
calculate_TST <- function(csv_file) {
  sleep_epochs <- csv_file %>% filter(Event %in% c('Stage1', 'Stage2', 'Stage3', 'REM'))
  
  #If none, return 0
  if (nrow(sleep_epochs) == 0) return(0)
  
  #Sum duration between epochs
  tst_sec <- sum(sleep_epochs$Computed_Duration_sec, na.rm = TRUE)
  tst_min <- tst_sec / 60
  
  return(tst_min)
}

tst_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ tibble(TST_min = calculate_TST(.x))) %>%
  ungroup()

#Identify subjects with < 240 minutes sleep for exclusion
bad_ids <- tst_table %>%
  filter(TST_min < 240) %>%
  pull(Subject_ID)

print(tst_table)
clean_psg_trim <- clean_psg_trim %>%
  filter(!Subject_ID %in% bad_ids)
cat("Removed", length(bad_ids), "subjects")
message("clean_psg_trim now contains ", n_distinct(clean_psg_trim$Subject_ID), " subjects.")
```
#REM Onset latency function
```{r REM onset, echo=FALSE}
calculate_REM_onset <- function(csv_file) {
  sleep_stages <- c('Stage1', 'Stage2', 'Stage3', 'REM')

  #Sleep onset = first sleep stage (any NREM/REM)
  sleep_onset_row <- which(csv_file$Event %in% sleep_stages)[1]
  if (is.na(sleep_onset_row)) return(NA)
  sleep_onset_time <- csv_file$Start_Time_full[sleep_onset_row]

  #Find first REM event after sleep onset
  rem_row <- which(csv_file$Event == 'REM' & seq_along(csv_file$Event) > sleep_onset_row)[1]
  if (is.na(rem_row)) return(NA)
  rem_time <- csv_file$Start_Time_full[rem_row]

  #REM latency in minutes
  rem_latency <- as.numeric(difftime(rem_time, sleep_onset_time, units = 'mins'))
  return(rem_latency)
}
rem_lat_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ tibble(REM_Latency_min = calculate_REM_onset(.x))) %>%
  ungroup()
print(rem_lat_table)
# Filter to subjects with invalid REM latency (i.e it doesn't exist, coudl change to stop above a certain high threshold also if preferable)
rem_lat_table_filtered <- rem_lat_table %>%
 filter(!is.na(REM_Latency_min))
excluded_subjects <- setdiff(unique(clean_psg_trim$Subject_ID), rem_lat_table_filtered$Subject_ID)
clean_psg_trim <- clean_psg_trim %>%
 filter(Subject_ID %in% rem_lat_table_filtered$Subject_ID)
cat(length(excluded_subjects), "excluded for no REM onset or REM latency")
message("clean_psg_trim now contains ", dplyr::n_distinct(clean_psg_trim$Subject_ID), " subjects.")
```
#Sleep onset latency function
```{r Sleep onset latency, echo=FALSE}
calculate_sol <- function(csv_file) {
  sleep_stages <- c("Stage1", "Stage2", "Stage3", "REM")
  #Find the index of the first sleep epoch
  sleep_onset_row <- which(csv_file$Event %in% sleep_stages)[1]
  if (is.na(sleep_onset_row)) return(NA)  #No sleep occurred
  #Subset to rows before that point
  pre_sleep <- csv_file[1:(sleep_onset_row - 1), ]
  #Filter to Wake epochs and sum their durations
  wake_duration_sec <- pre_sleep %>%
    filter(Event == "Wake") %>%
    pull(Computed_Duration_sec)
  sol_min <- sum(wake_duration_sec, na.rm = TRUE) / 60  #convert to minutes
  return(sol_min)
}
sol_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ tibble(SOL_min = calculate_sol(.x))) %>%
  ungroup()
print(sol_table)
weird_sol <- sol_table %>% filter(SOL_min > 30 | SOL_min == 0)
#update df, removing SOLs that suggest insomnia/hypersomnolence 
clean_psg_trim <- clean_psg_trim %>%
filter(!Subject_ID %in% weird_sol$Subject_ID)
cat(nrow(weird_sol), "excluded for immediate or excessive SOL")
message("clean_psg_trim now contains ", n_distinct(clean_psg_trim$Subject_ID), " subjects.")
```
#Time awake function
```{r time awake, echo=FALSE}
#this didn't end up in pipeline but could be handy later
calculate_time_awake <- function(csv_file) {
  #Subset the data for 'Wake' events
  wake_data <- subset(csv_file, Event == 'Wake')
  
  #Check if wake_data is empty
  if (nrow(wake_data) == 0) {
    return(0)  #Return 0 if no wake events are found
  }

  #Sum all wake epoch durations
  wake_time_seconds <- sum(wake_data$Computed_Duration_sec, na.rm = TRUE)
  #wake time to minutes
  wake_time_mins <- wake_time_seconds / 60
  
  return(wake_time_mins)
}
wake_time_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ tibble(time_awake_min = calculate_time_awake(.x))) %>%
  ungroup()

print(wake_time_table)
```
#n2 time and percent (% of TST)
```{r n2}
calculate_n2_time <- function(csv_file) {
  n2_data <- subset(csv_file, Event == 'Stage2')
  
  #Sum actual epoch durations (in seconds) and convert to minutes
  n2_time_sec <- sum(n2_data$Computed_Duration_sec, na.rm = TRUE)
  n2_time_mins <- n2_time_sec / 60
  
  return(n2_time_mins)
}

calculate_n2_percentage <- function(csv_file) {
  TST <- calculate_TST(csv_file)  #Should already be in minutes
  n2 <- calculate_n2_time(csv_file)
  
  #Return NA if TST is zero (to avoid division by zero in case of no stage x sleep)
  if (TST == 0) return(NA)
  
  n2_percentage <- (n2 / TST) * 100
  return(n2_percentage)
}
n2_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ tibble(N2_time_min = calculate_n2_time(.x),
                        N2_percent = calculate_n2_percentage(.x))) %>%
  ungroup()

print(n2_table)
```
#N1 time and percent
```{r N1}
calculate_n1_time <- function(csv_file) {
  n1_data <- subset(csv_file, Event == 'Stage1')
  
  #Sum actual epoch durations (in seconds) and convert to minutes
  n1_time_sec <- sum(n1_data$Computed_Duration_sec, na.rm = TRUE)
  n1_time_mins <- n1_time_sec / 60
  
  return(n1_time_mins)
}

calculate_n1_percentage <- function(csv_file) {
  TST <- calculate_TST(csv_file)  #Should already be in minutes
  n1 <- calculate_n1_time(csv_file)
  
  if (TST == 0) return(NA)
  
  n1_percentage <- (n1 / TST) * 100
  return(n1_percentage)
}

n1_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ tibble(N1_time_min = calculate_n1_time(.x),
                        N1_percent = calculate_n1_percentage(.x))) %>%
  ungroup()

print(n1_table)
```
#N3% function
```{r N3%, echo=FALSE}
calculate_n3_time <- function(csv_file) {
  n3_data <- subset(csv_file, Event == 'Stage3')
  
  #Sum actual epoch durations (in seconds) and convert to minutes
  n3_time_sec <- sum(n3_data$Computed_Duration_sec, na.rm = TRUE)
  n3_time_mins <- n3_time_sec / 60
  
  return(n3_time_mins)
}

calculate_n3_percentage <- function(csv_file) {
  TST <- calculate_TST(csv_file)  #Should already be in minutes
  n3 <- calculate_n3_time(csv_file)
  
  if (TST == 0) return(NA)
  
  n3_percentage <- (n3 / TST) * 100
  return(n3_percentage)
}

n3_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ tibble(N3_time_min = calculate_n3_time(.x),
                        N3_percent = calculate_n3_percentage(.x))) %>%
  ungroup()

print(n3_table)
```
#REM% function
```{r REM%, echo=FALSE}
calculate_rem_time <- function(csv_file) {
  rem_data <- subset(csv_file, Event == 'REM')
  #Sum actual epoch durations (in seconds) and convert to minutes
  rem_time_sec <- sum(rem_data$Computed_Duration_sec, na.rm = TRUE)
  rem_time_mins <- rem_time_sec / 60
  
  return(rem_time_mins)
}

calculate_rem_percentage <- function(csv_file) {
  TST <- calculate_TST(csv_file)
  rem <- calculate_rem_time(csv_file)
  
  if (TST == 0) return(NA)
  
  rem_percentage <- (rem / TST) * 100
  return(rem_percentage)
}

rem_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ tibble(REM_time_min = calculate_rem_time(.x),
                        REM_percent = calculate_rem_percentage(.x))) %>%
  ungroup()

print(rem_table)
```
#TIB function
```{r tib, echo=FALSE}
calculate_tib <- function(csv_file) {
  # Find first valid Wake (start of TIB)
  first_wake_row <- which(csv_file$Event == "Wake")[1]
  if (is.na(first_wake_row)) return(NA)
  first_wake_time <- csv_file$Start_Time_full[first_wake_row]
  
  # Find *last* Wake (end of TIB)
  last_wake_row <- tail(which(csv_file$Event == "Wake"), 1)
  if (is.na(last_wake_row)) return(NA)
  last_wake_time <- csv_file$Start_Time_full[last_wake_row]

  # TIB = time between first and last wake (in minutes)
  tib_mins <- as.numeric(difftime(last_wake_time, first_wake_time, units = "mins"))
  if (tib_mins <= 0) return(NA)

  return(tib_mins)
}
tib_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ tibble(TIB_min = calculate_tib(.x))) %>%
  ungroup()

```
#Sleep efficiency function
```{r sei, echo=FALSE}
calculate_sei <- function(csv_file) {
  #Calculate TST (Total Sleep Time in minutes)
  TST <- calculate_TST(csv_file)
  TIB <- calculate_tib(csv_file)
  
  #Handle division by zero or NA cases, shouldn't exist but just in case there's some really bad sleepers
  if (is.na(TST) || is.na(TIB) || TIB == 0) return(NA)
  
  #SEI = (TST / TIB) * 100
  SEI <- (TST / TIB) * 100
  
  return(SEI)
}

sei_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ tibble(sei = calculate_sei(.x))) %>% ungroup()

print(sei_table)

sei_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ tibble(SEI = calculate_sei(.x))) %>%
  ungroup()

#Filter for SEI between 70 and 100
valid_sei_subjects <- sei_table %>%
  filter(SEI >= 70, SEI <= 100) %>%
  pull(Subject_ID)

#Filter original PSG data in-place
clean_psg_trim <- clean_psg_trim %>%
  filter(Subject_ID %in% valid_sei_subjects)
excluded_subjects <- setdiff(unique(sei_table$Subject_ID), valid_sei_subjects)
cat("Excluded", length(excluded_subjects), "subjects with SEI < 70% or > 100%\n")
message("clean_psg_trim now contains ", n_distinct(clean_psg_trim$Subject_ID), " subjects.")
```
#Apnea checker 
```{r AHI}
#Load apnoea data and flag probable OSA
apnoea_flagged <- read_excel("C:/Users/sefarrell/Downloads/STAGES_questionnaire/Questionnaires/PSG_apnoea.xlsx") %>%
  select(Subject_ID = s_code, ahi) %>%
  distinct(Subject_ID, .keep_all = TRUE) %>%
  mutate(
    probable_OSA = ifelse(!is.na(ahi) & ahi > 30, 1, 0),
    osa_category = case_when(
      probable_OSA == 1 ~ "Probable OSA (AHI > 30)",
      probable_OSA == 0 ~ "No probable OSA",
      TRUE ~ NA_character_
    )
  )

#Merge OSA info
clean_psg_flagged <- clean_psg_trim %>%
  select(-matches("ahi|probable_OSA|osa_category")) %>%  # Remove any old OSA columns in case they are remaining from prev analysis
  left_join(apnoea_flagged, by = "Subject_ID")

#count unique subjects per category
osa_counts <- clean_psg_flagged %>%
  distinct(Subject_ID, osa_category) %>%
  count(osa_category)

print(osa_counts)

#Exclude probable OSA (AHI 30 or above, this could be changed depending on exlcusion criteria)
clean_psg_trim <- clean_psg_flagged %>%
  filter(probable_OSA != 1 | is.na(probable_OSA))

excluded_n <- clean_psg_flagged %>%
  distinct(Subject_ID, .keep_all = TRUE) %>%
  filter(probable_OSA == 1) %>%
  nrow()
 
cat("Excluded", excluded_n, "subjects with probable OSA (AHI > 30\n")
cat("Remaining subjects in clean_psg_trim:", n_distinct(clean_psg_trim$Subject_ID), "\n")
```
#% Main loop
```{r main loop, echo=FALSE}
#Make sure all these functions are defined:
#- calculate_TST
#- calculate_WASO
#- calculate_sol
#- calculate_REM_onset
#- calculate_n1_time
#- calculate_n1_percentage
#- calculate_n2_time
#- calculate_n2_percentage
#- calculate_n3_time
#- calculate_n3_percentage
#- calculate_rem_time
#- calculate_rem_percentage
#- calculate_tib
#- calculate_sei
psg_metrics_for_subject <- function(df) {
  tibble(
    TST_min        = calculate_TST(df),
    WASO_min       = calculate_WASO(df)$waso_min,
    REM_Lat_min    = calculate_REM_onset(df),
    N1_time_min    = calculate_n1_time(df),
    N1_percent     = calculate_n1_percentage(df),
   N2_time_min    = calculate_n2_time(df),
    N2_percent     = calculate_n2_percentage(df),
   N3_time_min    = calculate_n3_time(df),
    N3_percent     = calculate_n3_percentage(df),
   REM_time_min   = calculate_rem_time(df),
    REM_percent    = calculate_rem_percentage(df),
    TIB_min        = calculate_tib(df),
    SEI            = calculate_sei(df),
    SOL            = calculate_sol(df)
  )
}
psg_results_table <- clean_psg_trim %>%
  group_by(Subject_ID) %>%
  group_modify(~ psg_metrics_for_subject(.x)) %>%
  ungroup()
message("clean_psg_trim contains ", n_distinct(clean_psg_trim$Subject_ID), " subjects.")
```